---
output: html_document
---


```{r setup, echo=FALSE, message=FALSE, warning = FALSE}

library(tidyverse)
library(lubridate)
library(ggmap)
library(scales)

knitr::opts_chunk$set(fig.width = 12, fig.height = 10)
readr.show_progress=FALSE

sentiments <- read_delim(
                    "E:\\uni\\Cluster and Cloud Computing\\COMP90024-Project\\assignment2\\TwitterExplorer\\Analysis\\GenerateStats\\bin\\sentimentStats.csv",
                    ",", col_types = "cDlli", quote = '', progress = FALSE)

sdata <- filter(sentiments, YearMth >= ymd(20170701), YearMth <= ymd(20180331))

timeOfDay <- read_delim(
                    "E:\\uni\\Cluster and Cloud Computing\\COMP90024-Project\\assignment2\\TwitterExplorer\\Analysis\\GenerateStats\\bin\\sentimentTimeOfDayActivity.csv",
                    ",", quote = '', progress = FALSE)



```


## Sentiment Analysis

An existing sentiment analyser formulated for Social Media research was used to establish a baseline sentiment for all the available tweet data.
The specific analyser (VADER - _Valence Aware Dictionary and sEntiment Reasoner_) is a lexicon and rule-based sentiment analysis tool. 
The tool was choosen as is specifically attuned to sentiments expressed in social media. 

Configuration and code from multiple sources (https://github.com/cjhutto/vaderSentiment  and  https://github.com/codingupastorm/vadersharp) 
were merged.  A number of performance issue where addressed with the available implementations.

The analyser is capability generating  scores from multiple perspectives.
The **compound** score  provides a single unidimensional measure of sentiment for a given sentence.
The compound score is computed by summing the valence scores of each word in the lexicon, 
adjusted according to the rules, and then normalized to be between -1 (most extreme negative) 
and +1 (most extreme positive). 

It can be used standardized thresholds for classifying sentences as either positive, neutral, or negative. 
Typical threshold values (used with approach) are:

* positive sentiment: compound score >= 0.05
* neutral sentiment: (compound score > -0.05) and (compound score < 0.05)
* negative sentiment: compound score <= -0.05

 
Specifically, the scoring  process:
 
It covers all features we are meant to address:

* scores word for positive & negative associations (e.g. awesome +)  (e.g. dickhead -)  
* increased  sentiment with capitalised words (e.g. I LOVE my dog,  I HATE you)  
* increased positive sentiment for !!!!!   
* understands some double negatives (e.g. not bad)   
* understand old smiles   : )   ):<     - it has prescribed weighting for each of these combinations.  
* understands  emojis   â¤ðŸ’žðŸ˜ŠðŸ’™   -  emojis are converted to word equivalents before sentiment analysis is applied
  e.g. ðŸ˜Š   becomes _"smiling face with smiling eyes"_




### Explorative Statistics 



```{r echo=FALSE,warning = FALSE, fig.height = 6}
allCityTtls <- sdata %>%
     group_by(YearMth) %>%
     summarise(Tally = sum(Count)) %>%
     mutate(Location = "Total") %>%
     select(Location,YearMth,Tally)

byCityTtls <- sdata %>%
     group_by(Location, YearMth) %>%
     summarise(Tally = sum(Count)) %>%
      ungroup()



ggplot(data = byCityTtls, aes(x = YearMth, y = Tally, group = Location, colour = Location)) +     
     geom_line() +
     geom_point() +
     geom_line(data = allCityTtls, aes(x = YearMth, y = Tally, group = Location, colour = Location), size = 1, linetype = "longdash") +
     xlab('Month Year') +
     ylab('Average Sentiment') +
     scale_color_brewer(palette = 'Dark2') +
     scale_y_continuous(labels = comma) +
     scale_x_date(date_breaks = "1 month", date_labels = "%b %y") +
     labs(title="Average VADER sentiment of geo-located tweets by month")

```




```{r echo=FALSE,warning = FALSE,, fig.height = 6}

group_by(timeOfDay, Hour) %>%
    summarise(Tally = sum(Count)) %>%
    ggplot(aes(x = Hour, y = Tally)) +
    geom_col(fill='red4') +
     xlab('Hour of Day (LocalTime)') +
     ylab('Total volume of Tweets in hour [h,h+1) of day') +
     scale_y_continuous(labels = comma) +    
     labs(title="Variation in geo-located tweets volumes with time of day")

```



```{r echo=FALSE,warning = FALSE,, fig.height = 6}


dw <- group_by(timeOfDay, DayOfWeek) %>%
    summarise(Tally = sum(Count))
dw$DayOfWeek <- as.factor(dw$DayOfWeek)
dw$DayOfWeek <- factor(dw$DayOfWeek, levels(dw$DayOfWeek)[c(4, 2, 6, 7, 5, 1, 3)])


ggplot(data = dw, aes(x = DayOfWeek, y = Tally)) +
     geom_col(fill='red4') +
     xlab('Day of Week (LocalTime)') +
     ylab('Total Volume of Tweets in day of week') +
     scale_y_continuous(labels = comma) +    
     labs(title="Variation in geo-located tweets volumes with day of week")

```

